--E1

-Spark - Unified Compute Engine
-Unified (can support mltiple task - Analysis , cleaning & transforming)
-Compute engine (noting is stored by SPARK and processing memory is RAM)
-Master-Slave(Parallel data processing) Arch
-Libraries 

--E2

-Can process Structured and semi-structured data
-3V of bigdata (velocity, variety, volumne)
-ELT after bigdata
-Follows Distiributed Approach (horizontal Scaling, Economical, high availablility)

--E3-HADOOP

-Hadoop - Framework same as Spark
-HDFS - distributed file system
-Hive - DWH built on top of Hadoop
-Hive runs on top of Hadoop and uses HDFS to store its data
-Spark is better with bigger the data
-Hadoop is slower than SPARK because it write data back to disk and reads again from disk to in-memory.
-Hadoop cannot handle streaming but spark can
-Spark Api (high level - dataframe , sparkSQL,Streaming,ML lib & Graphx) (low level - RDD and DStream)
-Security in HADOOP is better than SPARK bcs of Kerberos Authentication via YARN
-Hence Spark relies on HDFS for ACL(Access Control List) and YARN for Authentication
-Spark relies on DAG(& RDD - Immutable) for Fault tolerance & hadoop relies on replication factor

--E4-Spark Ecosystem

-Usually high level api is preferred
-Cluster manager - YARN, MESOS, Kubernetes

--E5-Spark Arch
-Master is also part of cluster (If i've 10 nodes then 1 will becore master (YARN) and remaining can be used as Worker node)
-developer will run Spark via Spark SUBMIT with all the reqired resource
-Entire node will not get allocated to developer via YARN only asked will get allocated i.e worker node capacity(10 core & 20GB) and dev asked (5 core & 10GB) then only whatever that is asked will get allocated.
-PySpark is Python wrapper over Spark JAVA Api using Py4j library (JVM application master) 
-So while using Pyspark, application master node will get installed with 2 driver, Python and JVM whereas in case of java only JVM will get installed and worked node will have only JVM to carry on processing task
-But when UDF is being defined then worked node need 2 drivers, hence its advised to avoid UDF

--E6-Transformations & Actions
-Transformations - Lasy Evaluations (map,filter,fatmap,group by, reduceby,distinct, join, sortby)
-Narrow dependency Transformations - are independent of other Transformations and doesnot require data movemnet between partitions (filter, select, union, map)
-Wide dependency Transformations - opposite of Narrow (group by)
-Actions - Trigger Transformations(collect, count, first, take ,reduce, saveasTextfile,countby)








[
sparksession is higher-level abstraction that includes SparkContext, SQLcontext, hiveContext
unified api to work with structured data while SparkContext is more low-level and focussed on RDDs
i.e. SparkSession is the preferred and more modern way to interact with Spark, while SparkContext is the older, 
lower-level API.
]

[
# Create Spark session with specific configurations for compute and memory
spark = SparkSession.builder \
    .appName("SparkSessionWithResources") \
    .config("spark.executor.memory", "1g")  # Memory for each executor (e.g., 1GB)
    .config("spark.executor.cores", "1")  # Number of cores per executor (e.g., 1 cores)
    .config("spark.num.executors", "4")  # Total number of executors (e.g., 4 executors)
    .config("spark.driver.memory", "512m")  # Memory for the driver (e.g., 2GB)
    .config("spark.driver.cores", "1")  # Number of cores for the driver (e.g., 1 core)
    .getOrCreate()

minimum memory that can be allocated to EXECUTOR is 1GB and 1 core
minimum memory that can be allocated to DRIVER is 512MB and 1 core
]

[
# Create a SparkSession with resource allocation configurations
spark = SparkSession.builder \
    .appName("ResourceAllocationExample") \
    
    # Configure driver resources
    .config("spark.driver.memory", "2g")  # Driver memory (2 GB)
    .config("spark.driver.cores", "2")    # Driver cores (2 cores)
    
    # Configure executor resources
    .config("spark.executor.memory", "4g")  # Executor memory (4 GB)
    .config("spark.executor.cores", "2")    # Executor cores (2 cores)
    .config("spark.num.executors", "4")     # Number of executors (4 executors)
    
    # Configure Spark worker (optional, depends on cluster manager)
    .config("spark.deploy.mode", "cluster")  # Or use 'local' for local mode

    .getOrCreate()

# Check SparkContext resource usage
sc = spark.sparkContext
print("Total Cores allocated:", sc.defaultParallelism)
print("Spark Configuration Details:", sc.getConf().getAll())
]

[
spark.deploy.mode: This property defines the mode in which your Spark application will run.
client: The driver program (the Spark application controlling the job) runs on the machine from which the Spark job is submitted.
cluster: The driver program runs inside the Spark cluster itself, on one of the worker nodes.
]

[
    updating spark Session.builder with master configration restricting it with limited number of cores can cause error
    so better keep it local[*] or do not specify anything. It might get conflicted & cause error like 
    "Error: 'NoneType' object has no attribute 'sc'"
]
