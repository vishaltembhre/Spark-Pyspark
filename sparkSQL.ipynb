{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI is available at: http://LAPTOP-1DNSHCL1:4040\n"
     ]
    }
   ],
   "source": [
    "spark_ui_url = sc.uiWebUrl\n",
    "print(f\"Spark UI is available at: {spark_ui_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Industry_aggregation_NZSIOC: string (nullable = true)\n",
      " |-- Industry_code_NZSIOC: string (nullable = true)\n",
      " |-- Industry_name_NZSIOC: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Variable_code: string (nullable = true)\n",
      " |-- Variable_name: string (nullable = true)\n",
      " |-- Variable_category: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- Industry_code_ANZSIC06: string (nullable = true)\n",
      "\n",
      "Error: 'NoneType' object has no attribute 'show'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read the CSV file\n",
    "    csv_df = spark.read.csv(\"C:/Users/Vishal/spark-pyspark/data/annual-enterprise-survey-2023-financial-year-provisional.csv\", header=True, inferSchema=True) #dataframe\n",
    "\n",
    "    csv_df.createOrReplaceTempView(\"t1\")\n",
    "\n",
    "    result = spark.sql(\"SELECT count(*) FROM t1 LIMIT 10\")\n",
    "\n",
    "    result2 = spark.sql(\"select count(distinct Variable_name) from t1 /*order by year*/ limit 200;\")\n",
    "\n",
    "    # csv_df.select(\"Year\", \"year\").show() #column name is not case sensitive and can use select funtion to select the column\n",
    "\n",
    "    # csv_df.withColumnRenamed(\"year\", \"year1\").show(1) #rename the column name just while displaying the data\n",
    "\n",
    "    # csv_df.printSchema() #print the schema of the dataframe or table structure\n",
    "\n",
    "    # csv_df.filter((col(\"year\") > 2022) & (col(\"Industry_code_NZSIOC\") == 99999)).show(10) #filter the data based on the condition\n",
    "\n",
    "    # csv_df.groupBy(\"year\",\"Industry_code_NZSIOC\").count().show() #group the data based on the column\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Industry_code_NZSIOC\": \"count\"}).show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.filter((col(\"year\") == 2022) & (col(\"Industry_code_NZSIOC\") == 99999)).groupBy(\"year\", \"Industry_code_NZSIOC\").count().show() #filter the data based on the condition and group the data based on the column\n",
    "    \n",
    "    # csv_df.filter(col(\"year\") == 2023).groupBy(\"year\").pivot(\"Industry_code_NZSIOC\").count().show() #pivot the data based on the column\n",
    "    \n",
    "    csv_df = csv_df.withColumn(\"Value\", col(\"Value\").cast(\"int\"))\n",
    "\n",
    "    # csv_df.filter(col(\"year\") == 2023).groupBy(\"year\").pivot(\"Industry_code_NZSIOC\").sum(\"Value\").show() #pivot the data based on the column and sum the data\n",
    "    \n",
    "    # csv_df.sort(\"year\").show(5)\n",
    "    # csv_df.sort(col(\"year\").desc()).show(5) #sort the data based on the column\n",
    "\n",
    "    # csv_df.sort(\"year\", \"Industry_code_NZSIOC\").show(5)\n",
    "\n",
    "    # csv_df.sort(col(\"year\").desc(), col(\"Industry_code_NZSIOC\").desc()).show(5) #sort the data based on the column\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg(count(\"*\"), avg(\"Value\"), sum(\"value\")).show()#group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).sort(\"year\").show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).sort(col(\"year\").desc()).show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).sort(\"year\", \"Value\").show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    #print the data in order where most number of null or nan values are present in a row ? \n",
    "\n",
    "    # csv_df.filter(col(\"Value\").isNull() | isnan(col(\"Value\"))).show(3)\n",
    "\n",
    "    # csv_df.na.drop().show() #drop the rows where any of the column has null or nan values\n",
    "    \n",
    "    # csv_df.na.drop(subset=[\"Value\"]).show() #drop the rows where the specific column has null or nan values   \n",
    "\n",
    "    # csv_df.na.fill(0).show() #fill the null or nan values with 0\n",
    "\n",
    "    # csv_df.na.fill(\"unknown\").show() #fill the null or nan values with unknown\n",
    "\n",
    "    # csv_df.na.fill({\"Value\": 0}).show() #fill the null or nan values with 0\n",
    "\n",
    "    # csv_df.replace(\"unknown\", \"NA\", subset=[\"Year\"]).show()\n",
    "\n",
    "\n",
    "    # csv_df.select(\"Year\").distinct().show() #distinct values in the column\n",
    "\n",
    "    # csv_df.distinct().show(10)\n",
    "\n",
    "    # print(csv_df.distinct().count())\n",
    "\n",
    "    # csv_df.dropDuplicates().show() #drop the duplicate rows\n",
    "\n",
    "    # csv_df.dropDuplicates([\"Year\"]).show() #drop the duplicate rows based on the column\n",
    "\n",
    "    # csv_df.dropDuplicates([\"Year\", \"Industry_code_NZSIOC\"]).show() #drop the duplicate rows based on the column\n",
    "\n",
    "    #parse date and pick parts of the date\n",
    "\n",
    "    # csv_df.select(\"Year\", year(\"Year\"), month(\"Year\"), dayofmonth(\"Year\"), dayofyear(\"Year\"), weekofyear(\"Year\")).show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # csv_df.show(5)\n",
    "    csv_df.printSchema().show()\n",
    "    # print(type(csv_df))\n",
    "    # print(type(result))\n",
    "    # Show the result\n",
    "    # result.show()\n",
    "    # result2.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- batters: struct (nullable = true)\n",
      " |    |-- batter: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ppu: double (nullable = true)\n",
      " |-- topping: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # JSON_df = spark.read.json(\"C:/Users/Vishal/spark-pyspark/JSON_4_Analysis.json\") # This will not work as the JSON file is not in the correct format\n",
    "    # JSON_df = spark.read.json(r\"C:\\Users\\Vishal\\spark-pyspark\\JSON_4_Analysis.json\")\n",
    "    JSON_df = spark.read.option(\"multiLine\", True).json(\"C:/Users/Vishal/spark-pyspark/data/JSON_4_Analysis.json\")\n",
    "    NESTED_JSON_df = spark.read.option(\"multiLine\", True).json(\"C:/Users/Vishal/spark-pyspark/data/Nested_JSON.json\")\n",
    "\n",
    "    NESTED_JSON_df.printSchema() \n",
    "    # NESTED_JSON_df.show(truncate=False)\n",
    "\n",
    "    # JSON_df.printSchema()  \n",
    "    # JSON_df.show(3,truncate=False, vertical=True)\n",
    "\n",
    "    # exploded_df = NESTED_JSON_df.selectExpr(\"name\", \"topping\", \"explode(batters.batter.id) as batter_id\")\n",
    "    # exploded_df.show(truncate=False)\n",
    "\n",
    "    # print(type(JSON_df))\n",
    "    # JSON_df.select(\"name\", \"bio\").show(3)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           datetime|\n",
      "+-------------------+\n",
      "|2025-01-11 14:30:00|\n",
      "+-------------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#date time format manipulation\n",
    "try:\n",
    "    date_data = [(\"2025-01-11 14:30:00\",), (\"2024-12-25 09:15:45\",), (\"2023-11-30 23:59:59\",)]\n",
    "    columns = [\"datetime\"]\n",
    "\n",
    "    # Create DataFrame\n",
    "    date_df = spark.createDataFrame(date_data, columns)\n",
    "    # date_df.show()\n",
    "\n",
    "    # df_date2 = date_df.withColumn(\"date_only\", to_date(col(\"datetime\")))\n",
    "    # df_date2.show()\n",
    "\n",
    "    # df_timestamp = date_df.withColumn(\"timestamp\", to_timestamp(col(\"datetime\")))\n",
    "    # df_timestamp.show()\n",
    "\n",
    "    df_extracted = date_df.withColumn(\"year\", year(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"month\", month(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"day\", dayofmonth(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"hour\", hour(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"minute\", minute(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"second\", second(col(\"datetime\")))\n",
    "    # df_extracted.show()\n",
    "\n",
    "    df_formatted = date_df.withColumn(\"formatted_date\", date_format(col(\"datetime\"), \"MMMM dd, yyyy\"))\n",
    "    # df_formatted.show()\n",
    "\n",
    "    df_diff = date_df.withColumn(\"diff\", datediff(current_date(), col(\"datetime\")))\n",
    "    # df_diff.show()\n",
    "\n",
    "    df_adjusted = date_df.withColumn(\"date_plus_5_days\", date_add(col(\"datetime\"), 5)) \\\n",
    "                .withColumn(\"date_minus_5_days\", date_sub(col(\"datetime\"), 5)) \\\n",
    "                .withColumn(\"month_truncated\", trunc(col(\"datetime\"), \"month\")) \\\n",
    "                 .withColumn(\"year_truncated\", trunc(col(\"datetime\"), \"year\"))\n",
    "    # df_adjusted.show()\n",
    "\n",
    "    \n",
    "    df_filtered = date_df.filter((col(\"datetime\") >= \"2025-01-01\") & (col(\"datetime\") <= \"2025-12-31\"))\n",
    "    df_filtered.show()\n",
    "    print(type(df_filtered))\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-----+----------------------+----------+----+----------+--------------------+-----+--------------------+-----+-----+-----------+----------+---------+---------+---------+---------+------------------+-------------------+-----------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|Value|Industry_code_ANZSIC06|row_number|rank|dense_rank|        percent_rank|ntile|           cume_dist|  lag| lead|first_value|last_value|avg_value|sum_value|max_value|min_value|      stddev_value|     variance_value|count_value|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-----+----------------------+----------+----+----------+--------------------+-----+--------------------+-----+-----+-----------+----------+---------+---------+---------+---------+------------------+-------------------+-----------+\n",
      "|2020|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    0|   ANZSIC06 division A|         1|   1|         1|                 0.0|    1| 0.01466275659824047| NULL|    0|          0|         0|      0.0|      0.0|        0|        0|               0.0|                0.0|          5|\n",
      "|2019|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    0|   ANZSIC06 division A|         2|   1|         1|                 0.0|    1| 0.01466275659824047|    0|    0|          0|         0|      0.0|      0.0|        0|        0|               0.0|                0.0|          5|\n",
      "|2018|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    0|   ANZSIC06 division A|         3|   1|         1|                 0.0|    1| 0.01466275659824047|    0|    0|          0|         0|      0.0|      0.0|        0|        0|               0.0|                0.0|          5|\n",
      "|2015|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    0|   ANZSIC06 division A|         4|   1|         1|                 0.0|    1| 0.01466275659824047|    0|    0|          0|         0|      0.0|      0.0|        0|        0|               0.0|                0.0|          5|\n",
      "|2013|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    0|   ANZSIC06 division A|         5|   1|         1|                 0.0|    1| 0.01466275659824047|    0|    1|          0|         0|      0.0|      0.0|        0|        0|               0.0|                0.0|          5|\n",
      "|2022|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    1|   ANZSIC06 division A|         6|   6|         2|0.014705882352941176|    1| 0.02346041055718475|    0|    1|          0|         1|    0.375|      3.0|        1|        0|0.5175491695067657|0.26785714285714285|          8|\n",
      "|2021|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    1|   ANZSIC06 division A|         7|   6|         2|0.014705882352941176|    1| 0.02346041055718475|    1|    1|          0|         1|    0.375|      3.0|        1|        0|0.5175491695067657|0.26785714285714285|          8|\n",
      "|2017|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|    1|   ANZSIC06 division A|         8|   6|         2|0.014705882352941176|    1| 0.02346041055718475|    1|1,562|          0|         1|    0.375|      3.0|        1|        0|0.5175491695067657|0.26785714285714285|          8|\n",
      "|2013|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H11|        Depreciation|Financial perform...|1,562|   ANZSIC06 division A|         9|   9|         3|0.023529411764705882|    1|0.026392961876832845|    1|1,605|          0|     1,562|    0.375|      3.0|    1,562|        0|0.5175491695067657|0.26785714285714285|          9|\n",
      "|2014|                    Level 1|                  AA|Agriculture, Fore...|Dollars (millions)|          H11|        Depreciation|Financial perform...|1,605|   ANZSIC06 division A|        10|  10|         4|0.026470588235294117|    1| 0.02932551319648094|1,562|1,776|          0|     1,605|    0.375|      3.0|    1,605|        0|0.5175491695067657|0.26785714285714285|         10|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-----+----------------------+----------+----+----------+--------------------+-----+--------------------+-----+-----+-----------+----------+---------+---------+---------+---------+------------------+-------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#windows function \n",
    "try:\n",
    "\n",
    "    df = spark.read.csv(\"C:/Users/Vishal/spark-pyspark/data/annual-enterprise-survey-2023-financial-year-provisional.csv\", header=True, inferSchema=True)\n",
    "    df.createOrReplaceTempView(\"t1\")\n",
    "    result_df = spark.sql(\"SELECT * from t1\")\n",
    "    # result_df.show(5)\n",
    "    window_spec = Window.partitionBy(\"Industry_code_NZSIOC\").orderBy(\"Value\")\n",
    "\n",
    "    result_df.withColumn(\"row_number\", row_number().over(window_spec))\\\n",
    "        .withColumn(\"rank\", rank().over(window_spec))\\\n",
    "        .withColumn(\"dense_rank\", dense_rank().over(window_spec))\\\n",
    "        .withColumn(\"percent_rank\", percent_rank().over(window_spec))\\\n",
    "        .withColumn(\"ntile\", ntile(4).over(window_spec))\\\n",
    "        .withColumn(\"cume_dist\", cume_dist().over(window_spec))\\\n",
    "        .withColumn(\"lag\", lag(\"Value\", 1).over(window_spec))\\\n",
    "        .withColumn(\"lead\", lead(\"Value\", 1).over(window_spec))\\\n",
    "        .withColumn(\"first_value\", first(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"last_value\", last(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"avg_value\", avg(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"sum_value\", sum(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"max_value\", max(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"min_value\", min(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"stddev_value\", stddev(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"variance_value\", variance(\"Value\").over(window_spec))\\\n",
    "        .withColumn(\"count_value\", count(\"Value\").over(window_spec))\\\n",
    "        .show(10)\n",
    "    \n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           datetime|\n",
      "+-------------------+\n",
      "|2025-01-11 14:30:00|\n",
      "+-------------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#date time format manipulation\n",
    "try:\n",
    "    date_data = [(\"2025-01-11 14:30:00\",), (\"2024-12-25 09:15:45\",), (\"2023-11-30 23:59:59\",)]\n",
    "    columns = [\"datetime\"]\n",
    "\n",
    "    # Create DataFrame\n",
    "    date_df = spark.createDataFrame(date_data, columns)\n",
    "    # date_df.show()\n",
    "\n",
    "    # df_date2 = date_df.withColumn(\"date_only\", to_date(col(\"datetime\")))\n",
    "    # df_date2.show()\n",
    "\n",
    "    # df_timestamp = date_df.withColumn(\"timestamp\", to_timestamp(col(\"datetime\")))\n",
    "    # df_timestamp.show()\n",
    "\n",
    "    df_extracted = date_df.withColumn(\"year\", year(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"month\", month(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"day\", dayofmonth(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"hour\", hour(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"minute\", minute(col(\"datetime\"))) \\\n",
    "                 .withColumn(\"second\", second(col(\"datetime\")))\n",
    "    # df_extracted.show()\n",
    "\n",
    "    df_formatted = date_df.withColumn(\"formatted_date\", date_format(col(\"datetime\"), \"MMMM dd, yyyy\"))\n",
    "    # df_formatted.show()\n",
    "\n",
    "    df_diff = date_df.withColumn(\"diff\", datediff(current_date(), col(\"datetime\")))\n",
    "    # df_diff.show()\n",
    "\n",
    "    df_adjusted = date_df.withColumn(\"date_plus_5_days\", date_add(col(\"datetime\"), 5)) \\\n",
    "                .withColumn(\"date_minus_5_days\", date_sub(col(\"datetime\"), 5)) \\\n",
    "                .withColumn(\"month_truncated\", trunc(col(\"datetime\"), \"month\")) \\\n",
    "                 .withColumn(\"year_truncated\", trunc(col(\"datetime\"), \"year\"))\n",
    "    # df_adjusted.show()\n",
    "\n",
    "    \n",
    "    df_filtered = date_df.filter((col(\"datetime\") >= \"2025-01-01\") & (col(\"datetime\") <= \"2025-12-31\"))\n",
    "    df_filtered.show()\n",
    "    print(type(df_filtered))\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: An error occurred while calling o45.load.\n",
      ": java.lang.NoClassDefFoundError: scala/Product$class\n",
      "\tat com.databricks.spark.xml.XmlRelation.<init>(XmlRelation.scala:30)\n",
      "\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:89)\n",
      "\tat com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.ClassNotFoundException: scala.Product$class\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n",
      "\t... 21 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = spark.read.format(\"xml\") \\\n",
    "    .option(\"rowTag\", \"book\") \\\n",
    "    .load(\"C:/Users/Vishal/spark-pyspark/data/books.xml\")\n",
    "\n",
    "\n",
    "\n",
    "    df.printSchema()\n",
    "\n",
    "    df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.submitTime', '1737571505299'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.master', 'local[4]'), ('spark.driver.port', '55781'), ('spark.app.id', 'local-1737572124362'), ('spark.executor.id', 'driver'), ('spark.sql.warehouse.dir', 'file:/C:/Users/Vishal/spark-pyspark/spark-warehouse'), ('spark.driver.host', 'LAPTOP-1DNSHCL1'), ('spark.app.startTime', '1737572124298'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'SparkSQLApp'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Stop the Spark context\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vishal\\spark-pyspark\\sparkvenv\\Lib\\site-packages\\pyspark\\sql\\session.py:1799\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[0;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39mclearDefaultSession()\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mclearActiveSession()\n\u001b[0;32m   1801\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vishal\\spark-pyspark\\sparkvenv\\Lib\\site-packages\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[1;32mc:\\Users\\Vishal\\spark-pyspark\\sparkvenv\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\Vishal\\spark-pyspark\\sparkvenv\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\Vishal\\spark-pyspark\\sparkvenv\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\Vishal\\spark-pyspark\\sparkvenv\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# Stop the Spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
