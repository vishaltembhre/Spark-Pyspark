{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLApp\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI is available at: http://LAPTOP-1DNSHCL1:4040\n"
     ]
    }
   ],
   "source": [
    "spark_ui_url = sc.uiWebUrl\n",
    "print(f\"Spark UI is available at: {spark_ui_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category| Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+------+----------------------+\n",
      "|2023|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|930995|  ANZSIC06 division...|\n",
      "|2023|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|821630|  ANZSIC06 division...|\n",
      "|2023|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...| 84354|  ANZSIC06 division...|\n",
      "|2023|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...| 25010|  ANZSIC06 division...|\n",
      "|2023|                    Level 1|               99999|      All industries|Dollars (millions)|          H08|   Total expenditure|Financial perform...|832964|  ANZSIC06 division...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read the CSV file\n",
    "    csv_df = spark.read.csv(\"C:/Users/Vishal/spark-pyspark/data/annual-enterprise-survey-2023-financial-year-provisional.csv\", header=True, inferSchema=True) #dataframe\n",
    "\n",
    "    csv_df.createOrReplaceTempView(\"t1\")\n",
    "\n",
    "    result = spark.sql(\"SELECT count(*) FROM t1 LIMIT 10\")\n",
    "\n",
    "    result2 = spark.sql(\"select count(distinct Variable_name) from t1 /*order by year*/ limit 200;\")\n",
    "\n",
    "    csv_df.show(5)\n",
    "    # print(csv_df.printSchema())\n",
    "    print(type(csv_df))\n",
    "    print(type(result))\n",
    "    # Show the result\n",
    "    # result.show()\n",
    "    # result2.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|name         |topping                                                                                                                                  |batter_id|\n",
      "+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1001     |\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1002     |\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1003     |\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1004     |\n",
      "|Raised       |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5003, Chocolate}, {5004, Maple}]                                                          |1001     |\n",
      "|Old Fashioned|[{5001, None}, {5002, Glazed}, {5003, Chocolate}, {5004, Maple}]                                                                         |1001     |\n",
      "|Old Fashioned|[{5001, None}, {5002, Glazed}, {5003, Chocolate}, {5004, Maple}]                                                                         |1002     |\n",
      "+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # JSON_df = spark.read.json(\"C:/Users/Vishal/spark-pyspark/JSON_4_Analysis.json\") # This will not work as the JSON file is not in the correct format\n",
    "    # JSON_df = spark.read.json(r\"C:\\Users\\Vishal\\spark-pyspark\\JSON_4_Analysis.json\")\n",
    "    JSON_df = spark.read.option(\"multiLine\", True).json(\"C:/Users/Vishal/spark-pyspark/data/JSON_4_Analysis.json\")\n",
    "    NESTED_JSON_df = spark.read.option(\"multiLine\", True).json(\"C:/Users/Vishal/spark-pyspark/data/Nested_JSON.json\")\n",
    "\n",
    "    # NESTED_JSON_df.printSchema() \n",
    "    # NESTED_JSON_df.show(truncate=False)\n",
    "\n",
    "    # JSON_df.printSchema()  \n",
    "    # JSON_df.show(3,truncate=False, vertical=True)\n",
    "\n",
    "    # exploded_df = NESTED_JSON_df.selectExpr(\"name\", \"topping\", \"explode(batters.batter.id) as batter_id\")\n",
    "    # exploded_df.show(truncate=False)\n",
    "\n",
    "    # print(type(JSON_df))\n",
    "    # JSON_df.select(\"name\", \"bio\").show(3)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
