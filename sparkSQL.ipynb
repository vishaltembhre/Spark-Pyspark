{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLApp\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI is available at: http://LAPTOP-1DNSHCL1:4040\n"
     ]
    }
   ],
   "source": [
    "spark_ui_url = sc.uiWebUrl\n",
    "print(f\"Spark UI is available at: {spark_ui_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Industry_aggregation_NZSIOC: string (nullable = true)\n",
      " |-- Industry_code_NZSIOC: string (nullable = true)\n",
      " |-- Industry_name_NZSIOC: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Variable_code: string (nullable = true)\n",
      " |-- Variable_name: string (nullable = true)\n",
      " |-- Variable_category: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- Industry_code_ANZSIC06: string (nullable = true)\n",
      "\n",
      "Error: 'NoneType' object has no attribute 'show'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read the CSV file\n",
    "    csv_df = spark.read.csv(\"C:/Users/Vishal/spark-pyspark/data/annual-enterprise-survey-2023-financial-year-provisional.csv\", header=True, inferSchema=True) #dataframe\n",
    "\n",
    "    csv_df.createOrReplaceTempView(\"t1\")\n",
    "\n",
    "    result = spark.sql(\"SELECT count(*) FROM t1 LIMIT 10\")\n",
    "\n",
    "    result2 = spark.sql(\"select count(distinct Variable_name) from t1 /*order by year*/ limit 200;\")\n",
    "\n",
    "    # csv_df.select(\"Year\", \"year\").show() #column name is not case sensitive and can use select funtion to select the column\n",
    "\n",
    "    # csv_df.withColumnRenamed(\"year\", \"year1\").show(1) #rename the column name just while displaying the data\n",
    "\n",
    "    # csv_df.printSchema() #print the schema of the dataframe or table structure\n",
    "\n",
    "    # csv_df.filter((col(\"year\") > 2022) & (col(\"Industry_code_NZSIOC\") == 99999)).show(10) #filter the data based on the condition\n",
    "\n",
    "    # csv_df.groupBy(\"year\",\"Industry_code_NZSIOC\").count().show() #group the data based on the column\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Industry_code_NZSIOC\": \"count\"}).show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.filter((col(\"year\") == 2022) & (col(\"Industry_code_NZSIOC\") == 99999)).groupBy(\"year\", \"Industry_code_NZSIOC\").count().show() #filter the data based on the condition and group the data based on the column\n",
    "    \n",
    "    # csv_df.filter(col(\"year\") == 2023).groupBy(\"year\").pivot(\"Industry_code_NZSIOC\").count().show() #pivot the data based on the column\n",
    "    \n",
    "    csv_df = csv_df.withColumn(\"Value\", col(\"Value\").cast(\"int\"))\n",
    "\n",
    "    # csv_df.filter(col(\"year\") == 2023).groupBy(\"year\").pivot(\"Industry_code_NZSIOC\").sum(\"Value\").show() #pivot the data based on the column and sum the data\n",
    "    \n",
    "    # csv_df.sort(\"year\").show(5)\n",
    "    # csv_df.sort(col(\"year\").desc()).show(5) #sort the data based on the column\n",
    "\n",
    "    # csv_df.sort(\"year\", \"Industry_code_NZSIOC\").show(5)\n",
    "\n",
    "    # csv_df.sort(col(\"year\").desc(), col(\"Industry_code_NZSIOC\").desc()).show(5) #sort the data based on the column\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg(count(\"*\"), avg(\"Value\"), sum(\"value\")).show()#group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).sort(\"year\").show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).sort(col(\"year\").desc()).show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    # csv_df.groupBy(\"year\").agg({\"Value\": \"sum\"}).sort(\"year\", \"Value\").show() #group the data based on the column and aggregate the data\n",
    "\n",
    "    #print the data in order where most number of null or nan values are present in a row ? \n",
    "\n",
    "    # csv_df.filter(col(\"Value\").isNull() | isnan(col(\"Value\"))).show(3)\n",
    "\n",
    "    # csv_df.na.drop().show() #drop the rows where any of the column has null or nan values\n",
    "    \n",
    "    # csv_df.na.drop(subset=[\"Value\"]).show() #drop the rows where the specific column has null or nan values   \n",
    "\n",
    "    # csv_df.na.fill(0).show() #fill the null or nan values with 0\n",
    "\n",
    "    # csv_df.na.fill(\"unknown\").show() #fill the null or nan values with unknown\n",
    "\n",
    "    # csv_df.na.fill({\"Value\": 0}).show() #fill the null or nan values with 0\n",
    "\n",
    "    # csv_df.replace(\"unknown\", \"NA\", subset=[\"Year\"]).show()\n",
    "\n",
    "\n",
    "    # csv_df.select(\"Year\").distinct().show() #distinct values in the column\n",
    "\n",
    "    # csv_df.distinct().show(10)\n",
    "\n",
    "    # print(csv_df.distinct().count())\n",
    "\n",
    "    # csv_df.dropDuplicates().show() #drop the duplicate rows\n",
    "\n",
    "    # csv_df.dropDuplicates([\"Year\"]).show() #drop the duplicate rows based on the column\n",
    "\n",
    "    # csv_df.dropDuplicates([\"Year\", \"Industry_code_NZSIOC\"]).show() #drop the duplicate rows based on the column\n",
    "\n",
    "    #parse date and pick parts of the date\n",
    "\n",
    "    # csv_df.select(\"Year\", year(\"Year\"), month(\"Year\"), dayofmonth(\"Year\"), dayofyear(\"Year\"), weekofyear(\"Year\")).show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # csv_df.show(5)\n",
    "    csv_df.printSchema().show()\n",
    "    # print(type(csv_df))\n",
    "    # print(type(result))\n",
    "    # Show the result\n",
    "    # result.show()\n",
    "    # result2.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|name         |topping                                                                                                                                  |batter_id|\n",
      "+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1001     |\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1002     |\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1003     |\n",
      "|Cake         |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|1004     |\n",
      "|Raised       |[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5003, Chocolate}, {5004, Maple}]                                                          |1001     |\n",
      "|Old Fashioned|[{5001, None}, {5002, Glazed}, {5003, Chocolate}, {5004, Maple}]                                                                         |1001     |\n",
      "|Old Fashioned|[{5001, None}, {5002, Glazed}, {5003, Chocolate}, {5004, Maple}]                                                                         |1002     |\n",
      "+-------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # JSON_df = spark.read.json(\"C:/Users/Vishal/spark-pyspark/JSON_4_Analysis.json\") # This will not work as the JSON file is not in the correct format\n",
    "    # JSON_df = spark.read.json(r\"C:\\Users\\Vishal\\spark-pyspark\\JSON_4_Analysis.json\")\n",
    "    JSON_df = spark.read.option(\"multiLine\", True).json(\"C:/Users/Vishal/spark-pyspark/data/JSON_4_Analysis.json\")\n",
    "    NESTED_JSON_df = spark.read.option(\"multiLine\", True).json(\"C:/Users/Vishal/spark-pyspark/data/Nested_JSON.json\")\n",
    "\n",
    "    # NESTED_JSON_df.printSchema() \n",
    "    # NESTED_JSON_df.show(truncate=False)\n",
    "\n",
    "    # JSON_df.printSchema()  \n",
    "    # JSON_df.show(3,truncate=False, vertical=True)\n",
    "\n",
    "    # exploded_df = NESTED_JSON_df.selectExpr(\"name\", \"topping\", \"explode(batters.batter.id) as batter_id\")\n",
    "    # exploded_df.show(truncate=False)\n",
    "\n",
    "    # print(type(JSON_df))\n",
    "    # JSON_df.select(\"name\", \"bio\").show(3)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
