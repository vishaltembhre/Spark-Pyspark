{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat_minor": 5,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "### This notebook shows examples of using Glue on Spark for users familiar with pandas\nTo follow this example notebook, execute the cells in order.\nThe keyboard shortcut to execute the current cell and jump to the following is: Shift+Enter.\n\nTo delete cells no longer needed (including this one), you can use the context menu or use the Escape key (to exit any cell you might be in) and then press the d key twice. You can select multiple cells using Shift + Up/Down, to delete many quickly.\n\nThis example assumes the configured role has permission to read/write on the default catalog database and the s3 glue temporary folder, otherwise update the code or the permissions accordingly.",
			"metadata": {},
			"id": "8cb16da0"
		},
		{
			"cell_type": "markdown",
			"source": "####  Running the following cell will set up and start your interactive session.",
			"metadata": {},
			"id": "dc7cacd1"
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 120\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport boto3\nimport sys\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 120 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 120\nSession ID: ea9a0f9c-ba13-4576-93f0-6105e83f488a\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session ea9a0f9c-ba13-4576-93f0-6105e83f488a to get into ready status...\nSession ea9a0f9c-ba13-4576-93f0-6105e83f488a has been created.\n\n",
					"output_type": "stream"
				}
			],
			"id": "a3fea417"
		},
		{
			"cell_type": "code",
			"source": "# Optimize the data movement from pandas to Spark DataFrame and back\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n\n# You can define a distributed Spark DataFrame, to read the data in a distributed way and be able to process large data\n# Here it takes a bit of time because we ask it to infer schema, in practice could just let it set everything as string\n# and handle the schema manually\nsdf = spark.read.csv(\"s3://awsglue-datasets/examples/medicare/Medicare_Hospital_Provider.csv\", \n                     header=True, inferSchema=True)\n",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "7a4db5b0"
		},
		{
			"cell_type": "code",
			"source": "# The schema inference considered the dollar amounts as string due to the $ symbol\n# Also in the csv there are some header with extra spaces, we'll deal with that later\nsdf.printSchema()",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "7f867e92"
		},
		{
			"cell_type": "code",
			"source": "# The last 3 columns are dollar amounts, let's parse them into Decimal numbers for calculations\nlast_3cols = sdf.columns[-3:]\n# These transformations are just defined here, until we extract the data Spark won't do the work (lazy execution)\nfor col_name in last_3cols:\n    # Note: normally for monies it's better to use decimal but pandas doesn't support it\n    sdf = sdf.withColumn(col_name, regexp_replace(sdf[col_name], '\\$', '').cast('double'))\n\n# The zip code is not really a number\nsdf = sdf.withColumn('Provider Zip Code', sdf['Provider Zip Code'].cast('string'))",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "473bf9ac"
		},
		{
			"cell_type": "code",
			"source": "# Check the parsing is working fine\nsdf.show(n=10)",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "68372c4f"
		},
		{
			"cell_type": "code",
			"source": "# Let's say you are only interested in California\nsdf_ca = sdf.filter('`Provider State` == \"CA\"')\n\n# Now that we have narrowed down the data, it's small enough that we can convert into a native pandas DataFrame\n# Unlike sdf which reads data distributed and when needed, this pdf uses the driver memory to store the data\n# so is faster for smaller data as long as it fits\npdf_ca = sdf_ca.toPandas()\n\n# The column names in the csv have extra spaces, in pandas we can trim that easily\npdf_ca.columns = [c.strip() for c in pdf_ca.columns]",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "150b2e81"
		},
		{
			"cell_type": "code",
			"source": "# Check the pandas schema\npdf_ca.dtypes",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "4d69ff44"
		},
		{
			"cell_type": "code",
			"source": "# Explore the statistics of the numeric columns\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 5)\npd.set_option('max_colwidth', 30)\npdf_ca.describe()",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "42882f97"
		},
		{
			"cell_type": "code",
			"source": "# Plot a histogram on the notebook directly from pandas\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.title(\"Histogram of average Medicare payments\")\nplt.xlabel(\"Average payment in dollars\")\nhistogram = pdf_ca['Average Medicare Payments'].plot.hist(bins=12, alpha=0.5)\n%matplot plt",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "d0310bfc"
		},
		{
			"cell_type": "markdown",
			"source": "If you have more experience with the pandas APIs would rather use that instead of Spark DataFrame  \nBut notice that while we use native pandas, only the driver was doing work and the rest of the cluster is not used   \nThat's why we set the minimum size: *%number_of_workers 2*  \n\nOn Glue 4.0, you can get both distributed computed and the pandas syntax by using the \"pandas on Spark\" API, it's not yet 100% compatible but should work for most cases",
			"metadata": {},
			"id": "fcb425ef"
		},
		{
			"cell_type": "code",
			"source": "# The following cells will only work on Glue 4.0\n# psdf is a pandas on Spark DataFrame, uses the pandas API but the data and processing is distributed\n# this means it has higher latency but also can scale beyond a single node to handler larger data\npsdf = sdf.pandas_api()\npsdf.columns = [c.strip() for c in psdf.columns]\n# Statistics on the full dataset\npsdf.describe()",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "a74e182e"
		},
		{
			"cell_type": "code",
			"source": "# Operate the data in a distributed way but using pandas syntax\nrelevant_psdf = psdf[(psdf['Total Discharges'] > 100) & (psdf['Average Medicare Payments'] > 10000)]\nrelevant_psdf.groupby([\"Provider State\", \"Provider Id\"])[\"Average Medicare Payments\"].max()",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "fc32263a"
		},
		{
			"cell_type": "markdown",
			"source": "Note that in the previous cell the output is not equivalent than the same on native pandas, which displays the results grouped by the first column.   \nAlso running a sort before the groupby wouldn't work the same way, in this case the data is distributed so it's unsorted again when doing the groupby. ",
			"metadata": {},
			"id": "78801bf9"
		},
		{
			"cell_type": "code",
			"source": "# Convert back to Spark DataFrame if you want leverage the data saving features (for instance creating a catalog table)\n# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html\nrelevant_sdf = relevant_psdf.to_spark()\nrelevant_sdf.show()\n\n# Or go a step further and convert to DynamicFrame to use its sinks and features\n# https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame-writer.html\nrelevant_dynf = DynamicFrame.fromDF(relevant_sdf, glueContext, \"\")",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "a264ce60"
		}
	]
}